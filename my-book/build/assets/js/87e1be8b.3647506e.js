"use strict";(self.webpackChunkmy_book=self.webpackChunkmy_book||[]).push([[271],{672:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>a,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"module-4-vla/chapter-03-finetuning","title":"Chapter 3: Fine-tuning and Deployment","description":"Introduction","source":"@site/docs/module-4-vla/chapter-03-finetuning.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/chapter-03-finetuning","permalink":"/module-4-vla/chapter-03-finetuning","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 2: Using Pre-trained VLA Models","permalink":"/module-4-vla/chapter-02-using-pretrained"}}');var i=t(4848),s=t(8453);const a={sidebar_position:3},r="Chapter 3: Fine-tuning and Deployment",l={},d=[{value:"Introduction",id:"introduction",level:2},{value:"Why Fine-tune?",id:"why-fine-tune",level:2},{value:"Data Collection Pipeline",id:"data-collection-pipeline",level:2},{value:"1. Teleoperation Setup",id:"1-teleoperation-setup",level:3},{value:"2. Quality Control",id:"2-quality-control",level:3},{value:"3. Data Augmentation",id:"3-data-augmentation",level:3},{value:"Fine-tuning Process",id:"fine-tuning-process",level:2},{value:"1. Prepare Dataset",id:"1-prepare-dataset",level:3},{value:"2. Fine-tuning Loop",id:"2-fine-tuning-loop",level:3},{value:"3. Low-Rank Adaptation (LoRA)",id:"3-low-rank-adaptation-lora",level:3},{value:"Evaluation After Fine-tuning",id:"evaluation-after-fine-tuning",level:2},{value:"Benchmark on Test Set",id:"benchmark-on-test-set",level:3},{value:"Real-World Testing",id:"real-world-testing",level:3},{value:"Deployment Considerations",id:"deployment-considerations",level:2},{value:"1. Safety Wrappers",id:"1-safety-wrappers",level:3},{value:"2. Failure Recovery",id:"2-failure-recovery",level:3},{value:"3. Continuous Learning",id:"3-continuous-learning",level:3},{value:"Best Practices Summary",id:"best-practices-summary",level:2},{value:"Summary",id:"summary",level:2},{value:"Further Reading",id:"further-reading",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"chapter-3-fine-tuning-and-deployment",children:"Chapter 3: Fine-tuning and Deployment"})}),"\n",(0,i.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsx)(n.p,{children:"While pre-trained VLA models work well on common tasks, fine-tuning is essential for optimal performance on your specific robot, environment, and tasks. In this chapter, you'll learn how to collect demonstration data, fine-tune VLA models efficiently, and deploy them reliably on real robots."}),"\n",(0,i.jsx)(n.h2,{id:"why-fine-tune",children:"Why Fine-tune?"}),"\n",(0,i.jsx)(n.p,{children:"Pre-trained models may struggle with:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Robot-specific kinematics"}),": Different arm lengths, joint limits"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Custom objects"}),": Novel tools, products not in training data"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Unique environments"}),": Your lab, warehouse, or home layout"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Specialized tasks"}),": Industry-specific manipulation"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Fine-tuning bridges this gap with 10-1000 demonstrations"})," (vs millions for training from scratch)."]}),"\n",(0,i.jsx)(n.h2,{id:"data-collection-pipeline",children:"Data Collection Pipeline"}),"\n",(0,i.jsx)(n.h3,{id:"1-teleoperation-setup",children:"1. Teleoperation Setup"}),"\n",(0,i.jsx)(n.p,{children:"Collect demonstrations via human teleoperation:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import numpy as np\nimport time\nfrom datetime import datetime\nimport pickle\n\nclass DemonstrationRecorder:\n    def __init__(self, robot, camera):\n        self.robot = robot\n        self.camera = camera\n        self.demonstrations = []\n        self.current_demo = []\n\n    def start_recording(self, instruction):\n        """Start recording a new demonstration."""\n        print(f"Recording: \'{instruction}\'")\n        self.current_demo = {\n            \'instruction\': instruction,\n            \'frames\': [],\n            \'timestamp\': datetime.now()\n        }\n\n    def record_frame(self):\n        """Record single timestep."""\n        frame = {\n            \'image\': self.camera.get_rgb(),\n            \'robot_state\': self.robot.get_joint_positions(),\n            \'action\': self.robot.get_joint_velocities(),\n            \'gripper\': self.robot.get_gripper_state()\n        }\n        self.current_demo[\'frames\'].append(frame)\n\n    def stop_recording(self, success=True):\n        """Stop and save demonstration."""\n        if success:\n            self.demonstrations.append(self.current_demo)\n            print(f"Demo saved: {len(self.current_demo[\'frames\'])} frames")\n        else:\n            print("Demo discarded (failed)")\n\n        self.current_demo = []\n\n    def save_dataset(self, filename):\n        """Save all demonstrations to file."""\n        with open(filename, \'wb\') as f:\n            pickle.dump(self.demonstrations, f)\n        print(f"Saved {len(self.demonstrations)} demonstrations to {filename}")\n\n# Usage\nrecorder = DemonstrationRecorder(robot, camera)\n\n# Collect demonstration\nrecorder.start_recording("pick up the red block")\nfor _ in range(100):  # Record for ~10 seconds at 10 Hz\n    recorder.record_frame()\n    time.sleep(0.1)\nrecorder.stop_recording(success=True)\n\n# Save dataset\nrecorder.save_dataset("my_demonstrations.pkl")\n'})}),"\n",(0,i.jsx)(n.h3,{id:"2-quality-control",children:"2. Quality Control"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def filter_demonstrations(demos, min_length=10, max_length=500):\n    """\n    Remove bad demonstrations.\n\n    Criteria:\n    - Too short (likely failed immediately)\n    - Too long (likely got stuck)\n    - Low variance (robot didn\'t move)\n    """\n    filtered = []\n\n    for demo in demos:\n        frames = demo[\'frames\']\n\n        # Check length\n        if len(frames) < min_length or len(frames) > max_length:\n            continue\n\n        # Check robot moved\n        states = np.array([f[\'robot_state\'] for f in frames])\n        variance = np.var(states, axis=0).mean()\n\n        if variance < 0.001:  # Robot barely moved\n            continue\n\n        filtered.append(demo)\n\n    print(f"Kept {len(filtered)}/{len(demos)} demonstrations")\n    return filtered\n\n# Apply filtering\nclean_demos = filter_demonstrations(demonstrations)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"3-data-augmentation",children:"3. Data Augmentation"}),"\n",(0,i.jsx)(n.p,{children:"Increase dataset size with augmentation:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import imgaug.augmenters as iaa\n\ndef augment_demonstrations(demos, num_augmentations=3):\n    \"\"\"\n    Create augmented versions of demonstrations.\n\n    Augmentations:\n    - Color jitter\n    - Brightness/contrast\n    - Gaussian noise\n    - Random crops\n    \"\"\"\n    augmenter = iaa.Sequential([\n        iaa.Sometimes(0.5, iaa.Multiply((0.8, 1.2))),  # Brightness\n        iaa.Sometimes(0.5, iaa.Add((-20, 20))),        # Contrast\n        iaa.Sometimes(0.3, iaa.GaussianBlur(sigma=(0, 1.0))),\n        iaa.Sometimes(0.3, iaa.AdditiveGaussianNoise(scale=(0, 0.05*255)))\n    ])\n\n    augmented = []\n\n    for demo in demos:\n        # Original\n        augmented.append(demo)\n\n        # Augmented versions\n        for _ in range(num_augmentations):\n            aug_demo = demo.copy()\n            aug_demo['frames'] = []\n\n            for frame in demo['frames']:\n                aug_frame = frame.copy()\n                aug_frame['image'] = augmenter.augment_image(frame['image'])\n                aug_demo['frames'].append(aug_frame)\n\n            augmented.append(aug_demo)\n\n    print(f\"Augmented {len(demos)} \u2192 {len(augmented)} demonstrations\")\n    return augmented\n\n# Apply augmentation\naugmented_demos = augment_demonstrations(clean_demos, num_augmentations=2)\n"})}),"\n",(0,i.jsx)(n.h2,{id:"fine-tuning-process",children:"Fine-tuning Process"}),"\n",(0,i.jsx)(n.h3,{id:"1-prepare-dataset",children:"1. Prepare Dataset"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\n\nclass VLADataset(Dataset):\n    def __init__(self, demonstrations, processor):\n        self.demonstrations = demonstrations\n        self.processor = processor\n\n        # Flatten demonstrations into (state, action) pairs\n        self.samples = []\n        for demo in demonstrations:\n            for i, frame in enumerate(demo['frames']):\n                if i < len(demo['frames']) - 1:  # Have next action\n                    self.samples.append({\n                        'image': frame['image'],\n                        'instruction': demo['instruction'],\n                        'robot_state': frame['robot_state'],\n                        'action': demo['frames'][i+1]['action']  # Next action\n                    })\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        sample = self.samples[idx]\n\n        # Process image\n        image = Image.fromarray(sample['image'])\n        inputs = self.processor(\n            images=image,\n            text=sample['instruction'],\n            return_tensors=\"pt\"\n        )\n\n        # Add robot state\n        inputs['robot_state'] = torch.tensor(sample['robot_state'])\n        inputs['action'] = torch.tensor(sample['action'])\n\n        # Remove batch dimension\n        for key in inputs:\n            if isinstance(inputs[key], torch.Tensor):\n                inputs[key] = inputs[key].squeeze(0)\n\n        return inputs\n\n# Create dataset\nfrom openvla import OpenVLA\n\nmodel = OpenVLA.from_pretrained(\"openvla-7b\")\nprocessor = model.processor\n\ndataset = VLADataset(demonstrations, processor)\ndataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n\nprint(f\"Dataset size: {len(dataset)} samples\")\n"})}),"\n",(0,i.jsx)(n.h3,{id:"2-fine-tuning-loop",children:"2. Fine-tuning Loop"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nfrom torch.optim import AdamW\nfrom tqdm import tqdm\n\ndef finetune_vla(model, dataloader, epochs=10, lr=1e-5):\n    """\n    Fine-tune VLA model on custom demonstrations.\n\n    Args:\n        model: Pre-trained VLA model\n        dataloader: Training data\n        epochs: Number of training epochs\n        lr: Learning rate (small for fine-tuning)\n\n    Returns:\n        Fine-tuned model\n    """\n    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n    model = model.to(device)\n    model.train()\n\n    # Optimizer\n    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n\n    # Loss function\n    criterion = nn.MSELoss()\n\n    # Training loop\n    for epoch in range(epochs):\n        total_loss = 0\n        num_batches = 0\n\n        pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{epochs}")\n        for batch in pbar:\n            # Move to device\n            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v\n                     for k, v in batch.items()}\n\n            # Forward pass\n            outputs = model(\n                pixel_values=batch[\'pixel_values\'],\n                input_ids=batch[\'input_ids\'],\n                attention_mask=batch[\'attention_mask\'],\n                robot_state=batch[\'robot_state\']\n            )\n\n            # Compute loss\n            predicted_action = outputs.action\n            target_action = batch[\'action\']\n            loss = criterion(predicted_action, target_action)\n\n            # Backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            # Logging\n            total_loss += loss.item()\n            num_batches += 1\n            pbar.set_postfix({\'loss\': total_loss/num_batches})\n\n        avg_loss = total_loss / num_batches\n        print(f"Epoch {epoch+1}: Average Loss = {avg_loss:.4f}")\n\n    return model\n\n# Fine-tune\nfinetuned_model = finetune_vla(model, dataloader, epochs=10, lr=1e-5)\n\n# Save\ntorch.save(finetuned_model.state_dict(), "finetuned_vla.pth")\n'})}),"\n",(0,i.jsx)(n.h3,{id:"3-low-rank-adaptation-lora",children:"3. Low-Rank Adaptation (LoRA)"}),"\n",(0,i.jsx)(n.p,{children:"Efficient fine-tuning with LoRA (updates only small adapters):"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from peft import get_peft_model, LoraConfig, TaskType\n\ndef finetune_with_lora(model, dataloader, epochs=10):\n    """\n    Fine-tune using LoRA (parameter-efficient).\n\n    Advantages:\n    - 10-100x fewer parameters to train\n    - Faster training\n    - Less memory required\n    """\n    # Configure LoRA\n    lora_config = LoraConfig(\n        task_type=TaskType.SEQ_2_SEQ_LM,\n        r=8,  # Rank of adaptation matrices\n        lora_alpha=32,\n        lora_dropout=0.1,\n        target_modules=["q_proj", "v_proj"]  # Which layers to adapt\n    )\n\n    # Wrap model with LoRA\n    model = get_peft_model(model, lora_config)\n\n    print(f"Trainable parameters: {model.print_trainable_parameters()}")\n\n    # Train (same as before)\n    model = finetune_vla(model, dataloader, epochs=epochs)\n\n    return model\n\n# Use LoRA for efficient fine-tuning\nlora_model = finetune_with_lora(model, dataloader, epochs=20)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"evaluation-after-fine-tuning",children:"Evaluation After Fine-tuning"}),"\n",(0,i.jsx)(n.h3,{id:"benchmark-on-test-set",children:"Benchmark on Test Set"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"def evaluate_model(model, test_dataloader):\n    \"\"\"\n    Evaluate fine-tuned model.\n\n    Metrics:\n    - Action prediction error (MSE)\n    - Success rate on held-out tasks\n    \"\"\"\n    model.eval()\n    device = next(model.parameters()).device\n\n    total_error = 0\n    num_samples = 0\n\n    with torch.no_grad():\n        for batch in test_dataloader:\n            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v\n                     for k, v in batch.items()}\n\n            # Predict\n            outputs = model(\n                pixel_values=batch['pixel_values'],\n                input_ids=batch['input_ids'],\n                attention_mask=batch['attention_mask'],\n                robot_state=batch['robot_state']\n            )\n\n            # Compute error\n            predicted = outputs.action\n            target = batch['action']\n            error = torch.mean((predicted - target) ** 2)\n\n            total_error += error.item()\n            num_samples += 1\n\n    avg_error = total_error / num_samples\n    print(f\"Test Set MSE: {avg_error:.4f}\")\n\n    return avg_error\n\n# Evaluate\ntest_error = evaluate_model(finetuned_model, test_dataloader)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"real-world-testing",children:"Real-World Testing"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"def test_on_robot(model, test_tasks):\n    \"\"\"\n    Test fine-tuned model on real robot.\n\n    Returns:\n        success_rate: Fraction of successful task executions\n    \"\"\"\n    successes = 0\n    total = len(test_tasks)\n\n    for i, task in enumerate(test_tasks):\n        print(f\"\\nTest {i+1}/{total}: {task['instruction']}\")\n\n        # Reset environment\n        reset_environment(task['initial_state'])\n\n        # Execute with model\n        success = execute_task_with_model(model, task)\n\n        if success:\n            print(\"\u2713 Success\")\n            successes += 1\n        else:\n            print(\"\u2717 Failed\")\n\n    success_rate = successes / total\n    print(f\"\\nSuccess Rate: {success_rate:.1%}\")\n\n    return success_rate\n\n# Test\ntasks = [\n    {'instruction': 'pick up red block', 'initial_state': ...},\n    {'instruction': 'place block in container', 'initial_state': ...},\n    {'instruction': 'open drawer', 'initial_state': ...},\n]\n\nsuccess_rate = test_on_robot(finetuned_model, tasks)\n"})}),"\n",(0,i.jsx)(n.h2,{id:"deployment-considerations",children:"Deployment Considerations"}),"\n",(0,i.jsx)(n.h3,{id:"1-safety-wrappers",children:"1. Safety Wrappers"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class SafeVLAController:\n    \"\"\"\n    Wraps VLA model with safety checks.\n    \"\"\"\n    def __init__(self, model, robot):\n        self.model = model\n        self.robot = robot\n\n        # Safety limits\n        self.max_joint_velocity = 1.0  # rad/s\n        self.max_gripper_force = 50.0  # N\n        self.workspace_bounds = {\n            'x': (-0.5, 0.5),\n            'y': (-0.5, 0.5),\n            'z': (0.0, 1.0)\n        }\n\n    def predict_safe_action(self, image, instruction, robot_state):\n        # Get model prediction\n        action = self.model.predict(image, instruction, robot_state)\n\n        # Clip joint velocities\n        action[:7] = np.clip(action[:7], -self.max_joint_velocity, self.max_joint_velocity)\n\n        # Check workspace bounds (forward kinematics)\n        end_effector_pos = self.robot.forward_kinematics(robot_state + action[:7] * 0.1)\n\n        for axis, (min_val, max_val) in self.workspace_bounds.items():\n            idx = {'x': 0, 'y': 1, 'z': 2}[axis]\n            if not (min_val <= end_effector_pos[idx] <= max_val):\n                print(f\"Warning: Action would violate {axis} workspace limit\")\n                action[:7] *= 0.5  # Reduce velocity\n\n        return action\n\n# Usage\nsafe_controller = SafeVLAController(finetuned_model, robot)\naction = safe_controller.predict_safe_action(image, instruction, state)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"2-failure-recovery",children:"2. Failure Recovery"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class RobustVLAController:\n    """\n    VLA controller with failure detection and recovery.\n    """\n    def __init__(self, model, robot):\n        self.model = model\n        self.robot = robot\n        self.stuck_threshold = 10  # Frames without progress\n        self.stuck_counter = 0\n        self.previous_state = None\n\n    def execute_task(self, instruction):\n        print(f"Executing: {instruction}")\n\n        for step in range(1000):  # Max 1000 steps\n            # Get observation\n            image = self.robot.get_camera_image()\n            state = self.robot.get_joint_positions()\n\n            # Predict action\n            action = self.model.predict(image, instruction, state)\n\n            # Execute\n            self.robot.apply_action(action)\n\n            # Check for stuck\n            if self.previous_state is not None:\n                movement = np.linalg.norm(state - self.previous_state)\n\n                if movement < 0.001:  # Barely moved\n                    self.stuck_counter += 1\n                else:\n                    self.stuck_counter = 0\n\n            self.previous_state = state\n\n            # Recovery if stuck\n            if self.stuck_counter >= self.stuck_threshold:\n                print("Robot appears stuck. Attempting recovery...")\n                self.recover()\n                self.stuck_counter = 0\n\n            # Check task completion\n            if self.is_task_complete(image, instruction):\n                print("Task completed successfully!")\n                return True\n\n        print("Task timeout")\n        return False\n\n    def recover(self):\n        """Recovery behavior when stuck."""\n        # Retract arm slightly\n        current_state = self.robot.get_joint_positions()\n        retract_action = -0.1 * current_state  # Move toward zero position\n        self.robot.apply_action(retract_action)\n        time.sleep(1.0)\n\n    def is_task_complete(self, image, instruction):\n        # Use vision-language model to check completion\n        # Or simple heuristics\n        return False  # Implement based on your setup\n\n# Usage\ncontroller = RobustVLAController(finetuned_model, robot)\nsuccess = controller.execute_task("pick up the mug")\n'})}),"\n",(0,i.jsx)(n.h3,{id:"3-continuous-learning",children:"3. Continuous Learning"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class ContinuousLearningVLA:\n    """\n    VLA that improves from deployment experience.\n    """\n    def __init__(self, model):\n        self.model = model\n        self.new_demonstrations = []\n\n    def execute_with_logging(self, instruction):\n        # Execute task\n        demo = self.record_demonstration(instruction)\n\n        # Ask human: was this successful?\n        success = input("Was the task successful? (y/n): ").lower() == \'y\'\n\n        if success:\n            self.new_demonstrations.append(demo)\n            print(f"Collected {len(self.new_demonstrations)} new demonstrations")\n\n        # Periodically retrain\n        if len(self.new_demonstrations) >= 100:\n            print("Retraining model with new data...")\n            self.retrain()\n\n    def retrain(self):\n        # Fine-tune on new demonstrations\n        dataset = VLADataset(self.new_demonstrations, self.model.processor)\n        dataloader = DataLoader(dataset, batch_size=8)\n\n        self.model = finetune_vla(self.model, dataloader, epochs=5)\n\n        # Clear buffer\n        self.new_demonstrations = []\n\n        print("Model updated!")\n\n# Usage\ncontinuous_model = ContinuousLearningVLA(finetuned_model)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"best-practices-summary",children:"Best Practices Summary"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Data Quality > Quantity"}),": 100 good demos better than 1000 poor ones"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Start Small"}),": Fine-tune on 1 task, validate, then expand"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Regular Evaluation"}),": Test on robot frequently during training"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Safety First"}),": Always use safety wrappers in deployment"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Monitor Performance"}),": Track success rate over time"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Iterate"}),": Fine-tune \u2192 test \u2192 collect failures \u2192 re-fine-tune"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"In this chapter, you learned:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"How to collect high-quality demonstration data"}),"\n",(0,i.jsx)(n.li,{children:"Fine-tuning techniques (full fine-tuning and LoRA)"}),"\n",(0,i.jsx)(n.li,{children:"Evaluation methods for fine-tuned models"}),"\n",(0,i.jsx)(n.li,{children:"Deployment strategies with safety wrappers"}),"\n",(0,i.jsx)(n.li,{children:"Failure recovery and continuous learning"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"With these skills, you can take pre-trained VLA models and adapt them to your specific robotics applications, achieving high performance with minimal training data."}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Congratulations!"})," You've completed the AI-Powered Robotics book. You now understand:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"ROS 2 fundamentals and Python integration"}),"\n",(0,i.jsx)(n.li,{children:"Simulation with Gazebo, Unity, and Isaac Sim"}),"\n",(0,i.jsx)(n.li,{children:"Sensor modeling and synthetic data generation"}),"\n",(0,i.jsx)(n.li,{children:"Vision-Language-Action models for robot control"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"You're ready to build intelligent robotic systems that bridge AI and physical actuation!"}),"\n",(0,i.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:'LoRA Paper: Hu, E. J., et al. (2021). "LoRA: Low-Rank Adaptation of Large Language Models."'}),"\n",(0,i.jsx)(n.li,{children:'Behavioral Cloning: Pomerleau, D. A. (1988). "ALVINN: An Autonomous Land Vehicle in a Neural Network."'}),"\n",(0,i.jsxs)(n.li,{children:["Open X-Embodiment: ",(0,i.jsx)(n.a,{href:"https://robotics-transformer-x.github.io/",children:"https://robotics-transformer-x.github.io/"})]}),"\n",(0,i.jsx)(n.li,{children:'Best Practices: Google Robotics Team. "Robotics at Google: Best Practices."'}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Final Learning Check"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"\u2713 Collect demonstration data"}),"\n",(0,i.jsx)(n.li,{children:"\u2713 Fine-tune VLA models efficiently"}),"\n",(0,i.jsx)(n.li,{children:"\u2713 Evaluate model performance"}),"\n",(0,i.jsx)(n.li,{children:"\u2713 Deploy safely on real robots"}),"\n",(0,i.jsx)(n.li,{children:"\u2713 Implement continuous learning"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Next Steps"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Build your own robot project"}),"\n",(0,i.jsx)(n.li,{children:"Contribute to open-source robotics"}),"\n",(0,i.jsx)(n.li,{children:"Join the robotics research community"}),"\n",(0,i.jsx)(n.li,{children:"Keep learning and experimenting!"}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>r});var o=t(6540);const i={},s=o.createContext(i);function a(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);