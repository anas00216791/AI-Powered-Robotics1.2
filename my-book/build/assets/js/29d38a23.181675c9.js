"use strict";(self.webpackChunkmy_book=self.webpackChunkmy_book||[]).push([[767],{3855:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>u,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-4-vla/chapter-02-using-pretrained","title":"Chapter 2: Using Pre-trained VLA Models","description":"Introduction","source":"@site/docs/module-4-vla/chapter-02-using-pretrained.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/chapter-02-using-pretrained","permalink":"/module-4-vla/chapter-02-using-pretrained","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 1: Introduction to VLA Models","permalink":"/module-4-vla/chapter-01-introduction"},"next":{"title":"Chapter 3: Fine-tuning and Deployment","permalink":"/module-4-vla/chapter-03-finetuning"}}');var s=t(4848),o=t(8453);const r={sidebar_position:2},l="Chapter 2: Using Pre-trained VLA Models",a={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Available Pre-trained Models",id:"available-pre-trained-models",level:2},{value:"OpenVLA (Recommended for Getting Started)",id:"openvla-recommended-for-getting-started",level:3},{value:"RT-1 Checkpoints",id:"rt-1-checkpoints",level:3},{value:"Octo",id:"octo",level:3},{value:"Setting Up OpenVLA",id:"setting-up-openvla",level:2},{value:"Installation",id:"installation",level:3},{value:"Download Pre-trained Weights",id:"download-pre-trained-weights",level:3},{value:"Basic Inference",id:"basic-inference",level:2},{value:"Loading the Model",id:"loading-the-model",level:3},{value:"Single-Step Prediction",id:"single-step-prediction",level:3},{value:"Integration with ROS 2",id:"integration-with-ros-2",level:2},{value:"VLA Action Server",id:"vla-action-server",level:3},{value:"Usage",id:"usage",level:3},{value:"Real-Time Performance Optimization",id:"real-time-performance-optimization",level:2},{value:"Model Quantization",id:"model-quantization",level:3},{value:"Batching for Throughput",id:"batching-for-throughput",level:3},{value:"Evaluation and Testing",id:"evaluation-and-testing",level:2},{value:"Success Rate Measurement",id:"success-rate-measurement",level:3},{value:"Failure Analysis",id:"failure-analysis",level:3},{value:"Using Multiple Models",id:"using-multiple-models",level:2},{value:"Ensemble Predictions",id:"ensemble-predictions",level:3},{value:"Model Selection Based on Task",id:"model-selection-based-on-task",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Low Success Rate",id:"low-success-rate",level:3},{value:"Slow Inference",id:"slow-inference",level:3},{value:"Unexpected Actions",id:"unexpected-actions",level:3},{value:"Summary",id:"summary",level:2},{value:"Further Reading",id:"further-reading",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"chapter-2-using-pre-trained-vla-models",children:"Chapter 2: Using Pre-trained VLA Models"})}),"\n",(0,s.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsx)(n.p,{children:"Training VLA models from scratch requires massive datasets and computational resources. Fortunately, several pre-trained models are available that you can use immediately or fine-tune for your specific tasks. In this chapter, you'll learn how to deploy and use these models in your robotics projects."}),"\n",(0,s.jsx)(n.h2,{id:"available-pre-trained-models",children:"Available Pre-trained Models"}),"\n",(0,s.jsx)(n.h3,{id:"openvla-recommended-for-getting-started",children:"OpenVLA (Recommended for Getting Started)"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Open-source 7B parameter model"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Repository: ",(0,s.jsx)(n.a,{href:"https://github.com/openvla/openvla",children:"https://github.com/openvla/openvla"})]}),"\n",(0,s.jsx)(n.li,{children:"License: MIT (fully open)"}),"\n",(0,s.jsx)(n.li,{children:"Training Data: Open X-Embodiment (900K+ trajectories)"}),"\n",(0,s.jsx)(n.li,{children:"Hardware: Runs on single RTX 4090 or A100"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"rt-1-checkpoints",children:"RT-1 Checkpoints"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Google's robotics transformer"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Available through: TensorFlow Hub"}),"\n",(0,s.jsx)(n.li,{children:"Pre-trained on: 130K demonstrations, 700 tasks"}),"\n",(0,s.jsx)(n.li,{children:"Best for: Tabletop manipulation"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"octo",children:"Octo"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Open-source foundation model"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Repository: ",(0,s.jsx)(n.a,{href:"https://github.com/octo-models/octo",children:"https://github.com/octo-models/octo"})]}),"\n",(0,s.jsx)(n.li,{children:"Size: 93M parameters (efficient!)"}),"\n",(0,s.jsx)(n.li,{children:"Strength: Fast fine-tuning with limited data"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"setting-up-openvla",children:"Setting Up OpenVLA"}),"\n",(0,s.jsx)(n.h3,{id:"installation",children:"Installation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Create environment\nconda create -n openvla python=3.10\nconda activate openvla\n\n# Install PyTorch (adjust for your CUDA version)\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n\n# Install OpenVLA\ngit clone https://github.com/openvla/openvla\ncd openvla\npip install -e .\n\n# Install additional dependencies\npip install transformers accelerate\n"})}),"\n",(0,s.jsx)(n.h3,{id:"download-pre-trained-weights",children:"Download Pre-trained Weights"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Download model weights (~14 GB)\npython scripts/download_model.py --model openvla-7b\n\n# Model saved to: ./checkpoints/openvla-7b/\n"})}),"\n",(0,s.jsx)(n.h2,{id:"basic-inference",children:"Basic Inference"}),"\n",(0,s.jsx)(n.h3,{id:"loading-the-model",children:"Loading the Model"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import torch\nfrom openvla import OpenVLA\nfrom PIL import Image\nimport numpy as np\n\n# Load model\ndevice = torch.device("cuda" if torch.cuda.is_available() else "cpu")\nmodel = OpenVLA.from_pretrained("openvla-7b")\nmodel = model.to(device)\nmodel.eval()\n\nprint(f"Model loaded on {device}")\nprint(f"Parameters: {sum(p.numel() for p in model.parameters())/1e9:.2f}B")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"single-step-prediction",children:"Single-Step Prediction"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def predict_action(image, instruction, robot_state):\n    """\n    Predict robot action from observation.\n\n    Args:\n        image: PIL Image or numpy array (H, W, 3)\n        instruction: str, e.g., "pick up the red block"\n        robot_state: numpy array (7,) joint positions\n\n    Returns:\n        action: numpy array (8,) [7 joint velocities + gripper]\n    """\n    # Preprocess image\n    if isinstance(image, np.ndarray):\n        image = Image.fromarray(image)\n\n    # Model expects specific input format\n    inputs = model.processor(\n        images=image,\n        text=instruction,\n        return_tensors="pt"\n    ).to(device)\n\n    # Add robot state\n    inputs[\'robot_state\'] = torch.tensor(robot_state).unsqueeze(0).to(device)\n\n    # Predict\n    with torch.no_grad():\n        outputs = model(**inputs)\n        action = outputs.action.cpu().numpy()[0]\n\n    return action\n\n# Example usage\nimage = Image.open("camera_view.jpg")\ninstruction = "pick up the blue mug"\nrobot_state = np.array([0.0, -0.5, 0.0, -1.5, 0.0, 1.0, 0.0])  # Initial pose\n\naction = predict_action(image, instruction, robot_state)\nprint(f"Predicted action: {action}")\n'})}),"\n",(0,s.jsx)(n.h2,{id:"integration-with-ros-2",children:"Integration with ROS 2"}),"\n",(0,s.jsx)(n.h3,{id:"vla-action-server",children:"VLA Action Server"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, JointState\nfrom std_msgs.msg import String\nfrom control_msgs.msg import JointTrajectoryControllerAction\nfrom cv_bridge import CvBridge\n\nimport torch\nfrom openvla import OpenVLA\nimport numpy as np\n\nclass VLAActionServer(Node):\n    def __init__(self):\n        super().__init__('vla_action_server')\n\n        # Load model\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model = OpenVLA.from_pretrained(\"openvla-7b\")\n        self.model = self.model.to(self.device)\n        self.model.eval()\n\n        self.get_logger().info(f\"VLA model loaded on {self.device}\")\n\n        # Subscribers\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.image_callback,\n            10\n        )\n\n        self.joint_sub = self.create_subscription(\n            JointState,\n            '/joint_states',\n            self.joint_callback,\n            10\n        )\n\n        self.instruction_sub = self.create_subscription(\n            String,\n            '/vla/instruction',\n            self.instruction_callback,\n            10\n        )\n\n        # Publisher for commanded actions\n        self.action_pub = self.create_publisher(\n            JointTrajectoryControllerAction,\n            '/joint_trajectory_controller/action',\n            10\n        )\n\n        # State\n        self.latest_image = None\n        self.latest_joints = None\n        self.current_instruction = None\n        self.bridge = CvBridge()\n\n        # Control loop\n        self.timer = self.create_timer(0.1, self.control_loop)  # 10 Hz\n\n    def image_callback(self, msg):\n        self.latest_image = self.bridge.imgmsg_to_cv2(msg, 'rgb8')\n\n    def joint_callback(self, msg):\n        self.latest_joints = np.array(msg.position)\n\n    def instruction_callback(self, msg):\n        self.current_instruction = msg.data\n        self.get_logger().info(f\"New instruction: {self.current_instruction}\")\n\n    def control_loop(self):\n        # Check we have all required inputs\n        if (self.latest_image is None or\n            self.latest_joints is None or\n            self.current_instruction is None):\n            return\n\n        # Predict action\n        action = self.predict_action(\n            self.latest_image,\n            self.current_instruction,\n            self.latest_joints\n        )\n\n        # Publish action\n        self.publish_action(action)\n\n    def predict_action(self, image, instruction, robot_state):\n        # Convert numpy image to PIL\n        from PIL import Image as PILImage\n        pil_image = PILImage.fromarray(image)\n\n        # Prepare inputs\n        inputs = self.model.processor(\n            images=pil_image,\n            text=instruction,\n            return_tensors=\"pt\"\n        ).to(self.device)\n\n        inputs['robot_state'] = torch.tensor(robot_state).unsqueeze(0).to(self.device)\n\n        # Predict\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n            action = outputs.action.cpu().numpy()[0]\n\n        return action\n\n    def publish_action(self, action):\n        # Convert action to ROS message\n        # action: [7 joint velocities + gripper]\n        # (Implementation depends on your robot's control interface)\n\n        self.get_logger().info(f\"Publishing action: {action}\")\n\ndef main():\n    rclpy.init()\n    node = VLAActionServer()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h3,{id:"usage",children:"Usage"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Terminal 1: Run VLA server\nros2 run my_robot vla_action_server\n\n# Terminal 2: Send instruction\nros2 topic pub /vla/instruction std_msgs/String \"data: 'pick up the red block'\"\n\n# Terminal 3: Monitor actions\nros2 topic echo /joint_trajectory_controller/action\n"})}),"\n",(0,s.jsx)(n.h2,{id:"real-time-performance-optimization",children:"Real-Time Performance Optimization"}),"\n",(0,s.jsx)(n.h3,{id:"model-quantization",children:"Model Quantization"}),"\n",(0,s.jsx)(n.p,{children:"Reduce model size and increase speed:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from openvla import OpenVLA\nimport torch\n\n# Load model\nmodel = OpenVLA.from_pretrained("openvla-7b")\n\n# Quantize to 8-bit (2x speedup, minimal accuracy loss)\nmodel = torch.quantization.quantize_dynamic(\n    model,\n    {torch.nn.Linear},\n    dtype=torch.qint8\n)\n\n# Save quantized model\ntorch.save(model.state_dict(), "openvla-7b-int8.pth")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"batching-for-throughput",children:"Batching for Throughput"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def predict_batch(images, instructions, robot_states):\n    """\n    Predict actions for multiple observations in parallel.\n\n    Args:\n        images: list of PIL Images\n        instructions: list of strings\n        robot_states: numpy array (batch, 7)\n\n    Returns:\n        actions: numpy array (batch, 8)\n    """\n    inputs = model.processor(\n        images=images,\n        text=instructions,\n        return_tensors="pt",\n        padding=True\n    ).to(device)\n\n    inputs[\'robot_state\'] = torch.tensor(robot_states).to(device)\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n        actions = outputs.action.cpu().numpy()\n\n    return actions\n\n# Use when processing multiple robots or parallel rollouts\nimages = [image1, image2, image3]\ninstructions = ["pick block", "open drawer", "close gripper"]\nrobot_states = np.array([state1, state2, state3])\n\nactions = predict_batch(images, instructions, robot_states)\n'})}),"\n",(0,s.jsx)(n.h2,{id:"evaluation-and-testing",children:"Evaluation and Testing"}),"\n",(0,s.jsx)(n.h3,{id:"success-rate-measurement",children:"Success Rate Measurement"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def evaluate_model(model, test_tasks, num_trials=10):\n    """\n    Evaluate model success rate on test tasks.\n\n    Args:\n        model: VLA model\n        test_tasks: list of (image, instruction, robot_state) tuples\n        num_trials: attempts per task\n\n    Returns:\n        success_rate: float\n    """\n    successes = 0\n    total = 0\n\n    for task in test_tasks:\n        for trial in range(num_trials):\n            # Reset environment\n            reset_robot()\n\n            # Execute task\n            success = execute_task(model, task)\n\n            if success:\n                successes += 1\n            total += 1\n\n            print(f"Task {task[\'name\']}, Trial {trial}: {\'\u2713\' if success else \'\u2717\'}")\n\n    success_rate = successes / total\n    print(f"\\nOverall Success Rate: {success_rate:.1%}")\n\n    return success_rate\n\n# Example\ntest_tasks = [\n    {"name": "pick_cube", "image": img1, "instruction": "pick up cube", "state": state1},\n    {"name": "open_drawer", "image": img2, "instruction": "open drawer", "state": state2},\n]\n\nsuccess_rate = evaluate_model(model, test_tasks, num_trials=5)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"failure-analysis",children:"Failure Analysis"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def analyze_failures(model, failed_tasks):\n    """\n    Analyze why tasks failed.\n    """\n    failure_modes = {\n        "perception": 0,    # Wrong object detected\n        "manipulation": 0,  # Execution error\n        "language": 0,      # Misunderstood instruction\n        "other": 0\n    }\n\n    for task in failed_tasks:\n        # Check if model detected correct object\n        predicted_action = model.predict(task[\'image\'], task[\'instruction\'], task[\'state\'])\n\n        # Classify failure\n        if not task[\'object_detected\']:\n            failure_modes["perception"] += 1\n        elif task[\'collision\']:\n            failure_modes["manipulation"] += 1\n        elif task[\'wrong_object\']:\n            failure_modes["language"] += 1\n        else:\n            failure_modes["other"] += 1\n\n    print("Failure Modes:")\n    for mode, count in failure_modes.items():\n        print(f"  {mode}: {count}")\n\n    return failure_modes\n'})}),"\n",(0,s.jsx)(n.h2,{id:"using-multiple-models",children:"Using Multiple Models"}),"\n",(0,s.jsx)(n.h3,{id:"ensemble-predictions",children:"Ensemble Predictions"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def ensemble_predict(models, image, instruction, robot_state):\n    """\n    Average predictions from multiple models.\n    """\n    actions = []\n\n    for model in models:\n        action = model.predict(image, instruction, robot_state)\n        actions.append(action)\n\n    # Average predictions\n    ensemble_action = np.mean(actions, axis=0)\n\n    return ensemble_action\n\n# Load multiple models\nmodels = [\n    OpenVLA.from_pretrained("openvla-7b"),\n    OpenVLA.from_pretrained("octo-base"),\n]\n\naction = ensemble_predict(models, image, instruction, robot_state)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"model-selection-based-on-task",children:"Model Selection Based on Task"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def select_model(instruction):\n    """\n    Choose best model based on task type.\n    """\n    if "pick" in instruction or "grasp" in instruction:\n        return grasp_specialist_model\n    elif "open" in instruction or "close" in instruction:\n        return manipulation_model\n    else:\n        return general_purpose_model\n\n# Usage\nmodel = select_model("pick up the mug")\naction = model.predict(image, "pick up the mug", robot_state)\n'})}),"\n",(0,s.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Start with Demo Mode"}),": Test model predictions without executing on real robot"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Safety Limits"}),": Constrain action space to prevent damage"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Gradual Deployment"}),": Test on simple tasks before complex ones"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Monitor Performance"}),": Track success rate over time"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Collect Failure Cases"}),": Use failures to improve via fine-tuning"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,s.jsx)(n.h3,{id:"low-success-rate",children:"Low Success Rate"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Problem"}),": Model succeeds less than 50% of time\n",(0,s.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Check if task is in pre-training distribution"}),"\n",(0,s.jsx)(n.li,{children:"Verify image quality (lighting, resolution)"}),"\n",(0,s.jsx)(n.li,{children:"Ensure instruction is clear and specific"}),"\n",(0,s.jsx)(n.li,{children:"Fine-tune on your specific environment"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"slow-inference",children:"Slow Inference"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Problem"}),": Less than 3 Hz control rate\n",(0,s.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Use GPU (not CPU)"}),"\n",(0,s.jsx)(n.li,{children:"Quantize model to int8"}),"\n",(0,s.jsx)(n.li,{children:"Reduce image resolution"}),"\n",(0,s.jsx)(n.li,{children:"Use smaller model variant"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"unexpected-actions",children:"Unexpected Actions"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Problem"}),": Robot makes strange movements\n",(0,s.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Check coordinate frame conventions"}),"\n",(0,s.jsx)(n.li,{children:"Verify action scaling"}),"\n",(0,s.jsx)(n.li,{children:"Add action smoothing/filtering"}),"\n",(0,s.jsx)(n.li,{children:"Clip actions to safe ranges"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"In this chapter, you learned:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"How to install and load pre-trained VLA models"}),"\n",(0,s.jsx)(n.li,{children:"Basic inference and action prediction"}),"\n",(0,s.jsx)(n.li,{children:"Integration with ROS 2 for robot control"}),"\n",(0,s.jsx)(n.li,{children:"Performance optimization techniques"}),"\n",(0,s.jsx)(n.li,{children:"Evaluation and testing methodologies"}),"\n",(0,s.jsx)(n.li,{children:"Best practices for deployment"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"With pre-trained VLA models, you can quickly prototype robot applications without training from scratch. In the next chapter, you'll learn how to fine-tune these models for your specific tasks and robots."}),"\n",(0,s.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["OpenVLA Documentation: ",(0,s.jsx)(n.a,{href:"https://openvla.github.io/",children:"https://openvla.github.io/"})]}),"\n",(0,s.jsxs)(n.li,{children:["Octo Models: ",(0,s.jsx)(n.a,{href:"https://octo-models.github.io/",children:"https://octo-models.github.io/"})]}),"\n",(0,s.jsxs)(n.li,{children:["Open X-Embodiment: ",(0,s.jsx)(n.a,{href:"https://robotics-transformer-x.github.io/",children:"https://robotics-transformer-x.github.io/"})]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Learning Check"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"\u2713 Load and run pre-trained VLA models"}),"\n",(0,s.jsx)(n.li,{children:"\u2713 Integrate VLA with ROS 2"}),"\n",(0,s.jsx)(n.li,{children:"\u2713 Optimize model performance"}),"\n",(0,s.jsx)(n.li,{children:"\u2713 Evaluate model success rate"}),"\n",(0,s.jsx)(n.li,{children:"\u2713 Troubleshoot common issues"}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>l});var i=t(6540);const s={},o=i.createContext(s);function r(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);