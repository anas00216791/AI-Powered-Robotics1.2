"use strict";(self.webpackChunkmy_book=self.webpackChunkmy_book||[]).push([[112],{6266:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>t,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"module-2-gazebo-unity/chapter-03-sensors","title":"Chapter 3: Sensor Simulation","description":"Introduction","source":"@site/docs/module-2-gazebo-unity/chapter-03-sensors.md","sourceDirName":"module-2-gazebo-unity","slug":"/module-2-gazebo-unity/chapter-03-sensors","permalink":"/module-2-gazebo-unity/chapter-03-sensors","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 2: Unity for High-Fidelity Rendering","permalink":"/module-2-gazebo-unity/chapter-02-unity"},"next":{"title":"Module 3: NVIDIA Isaac Sim Integration","permalink":"/module-3-isaac-sim/"}}');var s=i(4848),r=i(8453);const t={sidebar_position:3},o="Chapter 3: Sensor Simulation",l={},d=[{value:"Introduction",id:"introduction",level:2},{value:"Types of Robotic Sensors",id:"types-of-robotic-sensors",level:2},{value:"Exteroceptive Sensors",id:"exteroceptive-sensors",level:3},{value:"Proprioceptive Sensors",id:"proprioceptive-sensors",level:3},{value:"LiDAR Simulation",id:"lidar-simulation",level:2},{value:"LiDAR in Gazebo",id:"lidar-in-gazebo",level:3},{value:"3D LiDAR (Velodyne-style)",id:"3d-lidar-velodyne-style",level:3},{value:"Camera Simulation",id:"camera-simulation",level:2},{value:"RGB Camera in Gazebo",id:"rgb-camera-in-gazebo",level:3},{value:"Depth Camera (RGB-D)",id:"depth-camera-rgb-d",level:3},{value:"IMU Simulation",id:"imu-simulation",level:2},{value:"IMU in Gazebo",id:"imu-in-gazebo",level:3},{value:"Sensor Simulation in Unity",id:"sensor-simulation-in-unity",level:2},{value:"LiDAR in Unity",id:"lidar-in-unity",level:3},{value:"Depth Camera in Unity",id:"depth-camera-in-unity",level:3},{value:"Sensor Noise Modeling",id:"sensor-noise-modeling",level:2},{value:"Why Add Noise?",id:"why-add-noise",level:3},{value:"Types of Sensor Noise",id:"types-of-sensor-noise",level:3},{value:"Multi-Sensor Fusion",id:"multi-sensor-fusion",level:2},{value:"Combining LiDAR and Camera",id:"combining-lidar-and-camera",level:3},{value:"Sensor Calibration",id:"sensor-calibration",level:2},{value:"Camera Calibration",id:"camera-calibration",level:3},{value:"Best Practices for Sensor Simulation",id:"best-practices-for-sensor-simulation",level:2},{value:"Summary",id:"summary",level:2},{value:"Further Reading",id:"further-reading",level:2}];function c(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"chapter-3-sensor-simulation",children:"Chapter 3: Sensor Simulation"})}),"\n",(0,s.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsx)(e.p,{children:"Sensors are the eyes and ears of a robot. Simulating sensors accurately is crucial for developing perception and navigation algorithms without requiring physical hardware. In this chapter, you'll learn how to simulate LiDAR, depth cameras, IMUs, and other sensors in both Gazebo and Unity."}),"\n",(0,s.jsx)(e.h2,{id:"types-of-robotic-sensors",children:"Types of Robotic Sensors"}),"\n",(0,s.jsx)(e.h3,{id:"exteroceptive-sensors",children:"Exteroceptive Sensors"}),"\n",(0,s.jsx)(e.p,{children:"Measure external environment:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"LiDAR"}),": Laser range finding for mapping and obstacle detection"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Cameras"}),": RGB images for object recognition and tracking"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Depth Cameras"}),": 3D perception (RGB-D sensors like RealSense, Kinect)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Ultrasonic"}),": Short-range distance measurement"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Radar"}),": Long-range detection and velocity measurement"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"proprioceptive-sensors",children:"Proprioceptive Sensors"}),"\n",(0,s.jsx)(e.p,{children:"Measure internal robot state:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"IMU"}),": Inertial Measurement Unit (acceleration, angular velocity)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Encoders"}),": Joint positions and velocities"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Force/Torque Sensors"}),": Interaction forces"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"GPS"}),": Global positioning (outdoor navigation)"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"lidar-simulation",children:"LiDAR Simulation"}),"\n",(0,s.jsx)(e.h3,{id:"lidar-in-gazebo",children:"LiDAR in Gazebo"}),"\n",(0,s.jsx)(e.p,{children:"Add a 2D LiDAR to your robot URDF:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:'<robot name="robot_with_lidar">\n\n  \x3c!-- Base link (your robot) --\x3e\n  <link name="base_link">\n    \x3c!-- Your robot definition --\x3e\n  </link>\n\n  \x3c!-- LiDAR link --\x3e\n  <link name="lidar_link">\n    <visual>\n      <geometry>\n        <cylinder radius="0.05" length="0.07"/>\n      </geometry>\n      <material name="black">\n        <color rgba="0 0 0 1"/>\n      </material>\n    </visual>\n  </link>\n\n  \x3c!-- Joint connecting LiDAR to robot --\x3e\n  <joint name="lidar_joint" type="fixed">\n    <parent link="base_link"/>\n    <child link="lidar_link"/>\n    <origin xyz="0 0 0.15" rpy="0 0 0"/>\n  </joint>\n\n  \x3c!-- Gazebo LiDAR plugin --\x3e\n  <gazebo reference="lidar_link">\n    <sensor name="lidar" type="ray">\n      <pose>0 0 0 0 0 0</pose>\n      <visualize>true</visualize>\n      <update_rate>10</update_rate>\n\n      <ray>\n        <scan>\n          <horizontal>\n            <samples>360</samples>\n            <resolution>1.0</resolution>\n            <min_angle>-3.14159</min_angle>\n            <max_angle>3.14159</max_angle>\n          </horizontal>\n        </scan>\n\n        <range>\n          <min>0.12</min>\n          <max>10.0</max>\n          <resolution>0.01</resolution>\n        </range>\n\n        <noise>\n          <type>gaussian</type>\n          <mean>0.0</mean>\n          <stddev>0.01</stddev>\n        </noise>\n      </ray>\n\n      <plugin name="lidar_plugin" filename="libgazebo_ros_ray_sensor.so">\n        <ros>\n          <remapping>~/out:=scan</remapping>\n        </ros>\n        <output_type>sensor_msgs/LaserScan</output_type>\n        <frame_name>lidar_link</frame_name>\n      </plugin>\n    </sensor>\n  </gazebo>\n\n</robot>\n'})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Testing the LiDAR"}),":"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"# Launch your robot with LiDAR\nros2 launch my_robot robot.launch.py\n\n# View scan data\nros2 topic echo /scan\n\n# Visualize in Rviz\nrviz2\n# Add > LaserScan > Topic: /scan\n"})}),"\n",(0,s.jsx)(e.h3,{id:"3d-lidar-velodyne-style",children:"3D LiDAR (Velodyne-style)"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:'<gazebo reference="velodyne_link">\n  <sensor name="velodyne" type="ray">\n    <pose>0 0 0 0 0 0</pose>\n    <visualize>false</visualize>\n    <update_rate>10</update_rate>\n\n    <ray>\n      <scan>\n        <horizontal>\n          <samples>1800</samples>\n          <resolution>1</resolution>\n          <min_angle>-3.14159</min_angle>\n          <max_angle>3.14159</max_angle>\n        </horizontal>\n        <vertical>\n          <samples>16</samples>\n          <resolution>1</resolution>\n          <min_angle>-0.2618</min_angle>\n          <max_angle>0.2618</max_angle>\n        </vertical>\n      </scan>\n\n      <range>\n        <min>0.9</min>\n        <max>130.0</max>\n        <resolution>0.001</resolution>\n      </range>\n    </ray>\n\n    <plugin name="velodyne_plugin" filename="libgazebo_ros_ray_sensor.so">\n      <ros>\n        <remapping>~/out:=velodyne_points</remapping>\n      </ros>\n      <output_type>sensor_msgs/PointCloud2</output_type>\n      <frame_name>velodyne_link</frame_name>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,s.jsx)(e.h2,{id:"camera-simulation",children:"Camera Simulation"}),"\n",(0,s.jsx)(e.h3,{id:"rgb-camera-in-gazebo",children:"RGB Camera in Gazebo"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:'<gazebo reference="camera_link">\n  <sensor name="camera" type="camera">\n    <pose>0 0 0 0 0 0</pose>\n    <visualize>true</visualize>\n    <update_rate>30</update_rate>\n\n    <camera>\n      <horizontal_fov>1.047</horizontal_fov>\n      <image>\n        <width>640</width>\n        <height>480</height>\n        <format>R8G8B8</format>\n      </image>\n      <clip>\n        <near>0.1</near>\n        <far>100</far>\n      </clip>\n\n      <noise>\n        <type>gaussian</type>\n        <mean>0.0</mean>\n        <stddev>0.007</stddev>\n      </noise>\n    </camera>\n\n    <plugin name="camera_plugin" filename="libgazebo_ros_camera.so">\n      <ros>\n        <remapping>image_raw:=camera/image_raw</remapping>\n        <remapping>camera_info:=camera/camera_info</remapping>\n      </ros>\n      <camera_name>camera</camera_name>\n      <frame_name>camera_link</frame_name>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,s.jsx)(e.h3,{id:"depth-camera-rgb-d",children:"Depth Camera (RGB-D)"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:'<gazebo reference="depth_camera_link">\n  <sensor name="depth_camera" type="depth">\n    <pose>0 0 0 0 0 0</pose>\n    <visualize>true</visualize>\n    <update_rate>30</update_rate>\n\n    <camera>\n      <horizontal_fov>1.047</horizontal_fov>\n      <image>\n        <width>640</width>\n        <height>480</height>\n      </image>\n      <clip>\n        <near>0.4</near>\n        <far>10.0</far>\n      </clip>\n    </camera>\n\n    <plugin name="depth_camera_plugin" filename="libgazebo_ros_camera.so">\n      <ros>\n        <remapping>image_raw:=depth/image_raw</remapping>\n        <remapping>depth/image_raw:=depth/image_raw</remapping>\n        <remapping>points:=depth/points</remapping>\n      </ros>\n      <camera_name>depth_camera</camera_name>\n      <frame_name>depth_camera_link</frame_name>\n      <min_depth>0.4</min_depth>\n      <max_depth>10.0</max_depth>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,s.jsx)(e.h2,{id:"imu-simulation",children:"IMU Simulation"}),"\n",(0,s.jsx)(e.h3,{id:"imu-in-gazebo",children:"IMU in Gazebo"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:'<gazebo reference="imu_link">\n  <sensor name="imu" type="imu">\n    <pose>0 0 0 0 0 0</pose>\n    <always_on>true</always_on>\n    <update_rate>100</update_rate>\n\n    <imu>\n      <angular_velocity>\n        <x>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.01</stddev>\n          </noise>\n        </x>\n        <y>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.01</stddev>\n          </noise>\n        </y>\n        <z>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.01</stddev>\n          </noise>\n        </z>\n      </angular_velocity>\n\n      <linear_acceleration>\n        <x>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.1</stddev>\n          </noise>\n        </x>\n        <y>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.1</stddev>\n          </noise>\n        </y>\n        <z>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.1</stddev>\n          </noise>\n        </z>\n      </linear_acceleration>\n    </imu>\n\n    <plugin name="imu_plugin" filename="libgazebo_ros_imu_sensor.so">\n      <ros>\n        <remapping>~/out:=imu/data</remapping>\n      </ros>\n      <frame_name>imu_link</frame_name>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Visualizing IMU Data"}),":"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Imu\n\nclass ImuVisualizer(Node):\n    def __init__(self):\n        super().__init__('imu_visualizer')\n        self.subscription = self.create_subscription(\n            Imu,\n            'imu/data',\n            self.imu_callback,\n            10)\n\n    def imu_callback(self, msg):\n        # Extract data\n        accel = msg.linear_acceleration\n        gyro = msg.angular_velocity\n\n        self.get_logger().info(\n            f'Accel: x={accel.x:.2f}, y={accel.y:.2f}, z={accel.z:.2f} | '\n            f'Gyro: x={gyro.x:.2f}, y={gyro.y:.2f}, z={gyro.z:.2f}'\n        )\n\ndef main():\n    rclpy.init()\n    node = ImuVisualizer()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(e.h2,{id:"sensor-simulation-in-unity",children:"Sensor Simulation in Unity"}),"\n",(0,s.jsx)(e.h3,{id:"lidar-in-unity",children:"LiDAR in Unity"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-csharp",children:'// LidarSensor.cs\nusing UnityEngine;\nusing Unity.Robotics.ROSTCPConnector;\nusing RosMessageTypes.Sensor;\n\npublic class LidarSensor : MonoBehaviour\n{\n    public int numRays = 360;\n    public float maxRange = 10.0f;\n    public float scanRate = 10.0f;\n    public string topicName = "/scan";\n\n    private ROSConnection ros;\n    private float timer;\n\n    void Start()\n    {\n        ros = ROSConnection.GetOrCreateInstance();\n        ros.RegisterPublisher<LaserScanMsg>(topicName);\n    }\n\n    void Update()\n    {\n        timer += Time.deltaTime;\n        if (timer >= 1.0f / scanRate)\n        {\n            PublishScan();\n            timer = 0;\n        }\n    }\n\n    void PublishScan()\n    {\n        float angleIncrement = 2 * Mathf.PI / numRays;\n        float[] ranges = new float[numRays];\n\n        for (int i = 0; i < numRays; i++)\n        {\n            float angle = -Mathf.PI + i * angleIncrement;\n            Vector3 direction = new Vector3(Mathf.Cos(angle), 0, Mathf.Sin(angle));\n\n            RaycastHit hit;\n            if (Physics.Raycast(transform.position, direction, out hit, maxRange))\n            {\n                ranges[i] = hit.distance;\n                Debug.DrawRay(transform.position, direction * hit.distance, Color.red);\n            }\n            else\n            {\n                ranges[i] = maxRange;\n                Debug.DrawRay(transform.position, direction * maxRange, Color.green);\n            }\n        }\n\n        LaserScanMsg msg = new LaserScanMsg\n        {\n            header = new RosMessageTypes.Std.HeaderMsg\n            {\n                stamp = new RosMessageTypes.BuiltinInterfaces.TimeMsg\n                {\n                    sec = (int)Time.time,\n                    nanosec = (uint)((Time.time % 1) * 1e9)\n                },\n                frame_id = gameObject.name\n            },\n            angle_min = -Mathf.PI,\n            angle_max = Mathf.PI,\n            angle_increment = angleIncrement,\n            time_increment = 0,\n            scan_time = 1.0f / scanRate,\n            range_min = 0.1f,\n            range_max = maxRange,\n            ranges = ranges\n        };\n\n        ros.Publish(topicName, msg);\n    }\n}\n'})}),"\n",(0,s.jsx)(e.h3,{id:"depth-camera-in-unity",children:"Depth Camera in Unity"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-csharp",children:'// DepthCamera.cs\nusing UnityEngine;\nusing Unity.Robotics.ROSTCPConnector;\nusing RosMessageTypes.Sensor;\n\n[RequireComponent(typeof(Camera))]\npublic class DepthCamera : MonoBehaviour\n{\n    public string topicName = "/depth/image_raw";\n    public float publishRate = 10.0f;\n    public float maxDepth = 10.0f;\n\n    private Camera depthCamera;\n    private ROSConnection ros;\n    private RenderTexture depthTexture;\n    private Texture2D texture2D;\n    private float timer;\n\n    void Start()\n    {\n        ros = ROSConnection.GetOrCreateInstance();\n        ros.RegisterPublisher<ImageMsg>(topicName);\n\n        depthCamera = GetComponent<Camera>();\n        depthCamera.depthTextureMode = DepthTextureMode.Depth;\n\n        depthTexture = new RenderTexture(640, 480, 24, RenderTextureFormat.Depth);\n        depthCamera.targetTexture = depthTexture;\n\n        texture2D = new Texture2D(640, 480, TextureFormat.RFloat, false);\n    }\n\n    void Update()\n    {\n        timer += Time.deltaTime;\n        if (timer >= 1.0f / publishRate)\n        {\n            PublishDepthImage();\n            timer = 0;\n        }\n    }\n\n    void PublishDepthImage()\n    {\n        RenderTexture.active = depthTexture;\n        texture2D.ReadPixels(new Rect(0, 0, 640, 480), 0, 0);\n        texture2D.Apply();\n\n        byte[] depthData = texture2D.GetRawTextureData();\n\n        ImageMsg msg = new ImageMsg\n        {\n            header = new RosMessageTypes.Std.HeaderMsg\n            {\n                stamp = new RosMessageTypes.BuiltinInterfaces.TimeMsg\n                {\n                    sec = (int)Time.time,\n                    nanosec = (uint)((Time.time % 1) * 1e9)\n                },\n                frame_id = gameObject.name\n            },\n            height = 480,\n            width = 640,\n            encoding = "32FC1",\n            step = 640 * 4,\n            data = depthData\n        };\n\n        ros.Publish(topicName, msg);\n    }\n}\n'})}),"\n",(0,s.jsx)(e.h2,{id:"sensor-noise-modeling",children:"Sensor Noise Modeling"}),"\n",(0,s.jsx)(e.h3,{id:"why-add-noise",children:"Why Add Noise?"}),"\n",(0,s.jsx)(e.p,{children:"Real sensors are never perfect. Adding realistic noise to simulated sensors:"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Tests algorithm robustness"}),"\n",(0,s.jsx)(e.li,{children:"Prevents overfitting to perfect data"}),"\n",(0,s.jsx)(e.li,{children:"Improves sim-to-real transfer"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"types-of-sensor-noise",children:"Types of Sensor Noise"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Gaussian Noise"}),": Random fluctuations"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"import numpy as np\n\n# Add Gaussian noise to range measurement\ntrue_range = 5.0\nnoise_stddev = 0.05\nnoisy_range = true_range + np.random.normal(0, noise_stddev)\n"})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Quantization Noise"}),": Discrete measurement levels"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"# Simulate 8-bit depth sensor (256 levels)\nmax_depth = 10.0\ntrue_depth = 3.45\nquantized_depth = np.round(true_depth / max_depth * 255) / 255 * max_depth\n# Result: 3.45 \u2192 3.4510 (closest 8-bit value)\n"})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Outliers"}),": Occasional bad measurements"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"# 1% chance of outlier\nif np.random.random() < 0.01:\n    measurement = np.random.uniform(0, max_range)  # Random outlier\nelse:\n    measurement = true_value + np.random.normal(0, noise_stddev)\n"})}),"\n",(0,s.jsx)(e.h2,{id:"multi-sensor-fusion",children:"Multi-Sensor Fusion"}),"\n",(0,s.jsx)(e.h3,{id:"combining-lidar-and-camera",children:"Combining LiDAR and Camera"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, Image\nfrom cv_bridge import CvBridge\nimport numpy as np\n\nclass SensorFusion(Node):\n    def __init__(self):\n        super().__init__('sensor_fusion')\n\n        self.lidar_sub = self.create_subscription(\n            LaserScan, '/scan', self.lidar_callback, 10)\n\n        self.image_sub = self.create_subscription(\n            Image, '/camera/image_raw', self.image_callback, 10)\n\n        self.latest_scan = None\n        self.latest_image = None\n        self.bridge = CvBridge()\n\n    def lidar_callback(self, msg):\n        self.latest_scan = msg\n        self.process_fusion()\n\n    def image_callback(self, msg):\n        self.latest_image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\n        self.process_fusion()\n\n    def process_fusion(self):\n        if self.latest_scan is None or self.latest_image is None:\n            return\n\n        # Find closest obstacle in LiDAR\n        min_range = min(self.latest_scan.ranges)\n        min_index = self.latest_scan.ranges.index(min_range)\n\n        angle = self.latest_scan.angle_min + min_index * self.latest_scan.angle_increment\n\n        self.get_logger().info(\n            f'Closest obstacle: {min_range:.2f}m at {np.degrees(angle):.1f}\xb0'\n        )\n\n        # Project to image (simplified - assumes aligned sensors)\n        # In real systems, use calibrated projection matrix\n        image_x = int((angle + np.pi) / (2 * np.pi) * self.latest_image.shape[1])\n\n        # Mark on image (for visualization)\n        # cv2.circle(self.latest_image, (image_x, 240), 10, (0, 0, 255), -1)\n\ndef main():\n    rclpy.init()\n    node = SensorFusion()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(e.h2,{id:"sensor-calibration",children:"Sensor Calibration"}),"\n",(0,s.jsx)(e.h3,{id:"camera-calibration",children:"Camera Calibration"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport numpy as np\nimport cv2\n\n# Camera intrinsic parameters\nfx = 525.0  # Focal length x (pixels)\nfy = 525.0  # Focal length y (pixels)\ncx = 319.5  # Principal point x\ncy = 239.5  # Principal point y\n\ncamera_matrix = np.array([\n    [fx, 0, cx],\n    [0, fy, cy],\n    [0, 0, 1]\n])\n\n# Distortion coefficients\ndist_coeffs = np.array([0.1, -0.15, 0, 0, 0])\n\n# Project 3D point to 2D image\ndef project_point(point_3d, camera_matrix, dist_coeffs):\n    """\n    Project a 3D point to 2D image coordinates.\n\n    Args:\n        point_3d: [x, y, z] in camera frame\n        camera_matrix: 3x3 intrinsic matrix\n        dist_coeffs: Distortion coefficients\n\n    Returns:\n        [u, v] image coordinates\n    """\n    point_3d = np.array(point_3d).reshape(1, 1, 3)\n    rvec = np.zeros((3, 1))\n    tvec = np.zeros((3, 1))\n\n    point_2d, _ = cv2.projectPoints(\n        point_3d, rvec, tvec, camera_matrix, dist_coeffs\n    )\n\n    return point_2d[0][0]\n\n# Example usage\npoint_3d = [1.0, 0.5, 3.0]  # 1m right, 0.5m up, 3m forward\npoint_2d = project_point(point_3d, camera_matrix, dist_coeffs)\nprint(f"3D point {point_3d} projects to 2D point {point_2d}")\n'})}),"\n",(0,s.jsx)(e.h2,{id:"best-practices-for-sensor-simulation",children:"Best Practices for Sensor Simulation"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Match Real Sensor Specs"}),": Use manufacturer datasheets for:"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Field of view"}),"\n",(0,s.jsx)(e.li,{children:"Range limits"}),"\n",(0,s.jsx)(e.li,{children:"Update rates"}),"\n",(0,s.jsx)(e.li,{children:"Resolution"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Add Realistic Noise"}),": Don't use perfect measurements\u2014real sensors are noisy"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Model Failure Modes"}),": Sensors fail in predictable ways:"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"LiDAR: Reflective surfaces cause bad readings"}),"\n",(0,s.jsx)(e.li,{children:"Cameras: Glare from bright lights"}),"\n",(0,s.jsx)(e.li,{children:"IMU: Drift over time"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Validate Against Real Data"}),": Compare simulated and real sensor outputs"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Start Simple"}),": Begin with perfect sensors, add complexity gradually"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(e.p,{children:"In this chapter, you learned:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"How to simulate LiDAR sensors in Gazebo and Unity"}),"\n",(0,s.jsx)(e.li,{children:"How to add RGB and depth cameras to your robots"}),"\n",(0,s.jsx)(e.li,{children:"How to simulate IMUs for robot state estimation"}),"\n",(0,s.jsx)(e.li,{children:"How to model realistic sensor noise and failures"}),"\n",(0,s.jsx)(e.li,{children:"How to perform multi-sensor fusion"}),"\n",(0,s.jsx)(e.li,{children:"Best practices for sensor calibration and validation"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:"Sensor simulation is critical for developing robust perception systems. With accurate sensor models, you can develop and test algorithms in simulation before deploying to real robots\u2014saving time and reducing hardware wear."}),"\n",(0,s.jsx)(e.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["Gazebo Sensor Documentation: ",(0,s.jsx)(e.a,{href:"http://gazebosim.org/tutorials?cat=sensors",children:"http://gazebosim.org/tutorials?cat=sensors"})]}),"\n",(0,s.jsxs)(e.li,{children:["ROS 2 Sensor Messages: ",(0,s.jsx)(e.a,{href:"https://github.com/ros2/common_interfaces/tree/rolling/sensor_msgs",children:"https://github.com/ros2/common_interfaces/tree/rolling/sensor_msgs"})]}),"\n",(0,s.jsxs)(e.li,{children:["OpenCV Camera Calibration: ",(0,s.jsx)(e.a,{href:"https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html",children:"https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html"})]}),"\n",(0,s.jsxs)(e.li,{children:["Point Cloud Library (PCL): ",(0,s.jsx)(e.a,{href:"https://pointclouds.org/",children:"https://pointclouds.org/"})]}),"\n"]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Learning Check"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"\u2713 Configure LiDAR sensors in simulation"}),"\n",(0,s.jsx)(e.li,{children:"\u2713 Add cameras (RGB and depth) to robots"}),"\n",(0,s.jsx)(e.li,{children:"\u2713 Simulate IMU sensors with realistic noise"}),"\n",(0,s.jsx)(e.li,{children:"\u2713 Understand sensor noise modeling"}),"\n",(0,s.jsx)(e.li,{children:"\u2713 Implement basic sensor fusion"}),"\n",(0,s.jsx)(e.li,{children:"\u2713 Calibrate simulated sensors"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Module 2 Complete!"})," You now understand physics simulation (Gazebo), rendering (Unity), and sensor modeling\u2014the complete simulation toolkit for robotics development."]})]})}function m(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(c,{...n})}):c(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>t,x:()=>o});var a=i(6540);const s={},r=a.createContext(s);function t(n){const e=a.useContext(r);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:t(n.components),a.createElement(r.Provider,{value:e},n.children)}}}]);