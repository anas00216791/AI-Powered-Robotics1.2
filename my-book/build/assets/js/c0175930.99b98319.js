"use strict";(self.webpackChunkmy_book=self.webpackChunkmy_book||[]).push([[937],{4732:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>s,default:()=>h,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-3-isaac-sim/chapter-03-synthetic-data","title":"Chapter 3: Synthetic Data Generation","description":"Introduction","source":"@site/docs/module-3-isaac-sim/chapter-03-synthetic-data.md","sourceDirName":"module-3-isaac-sim","slug":"/module-3-isaac-sim/chapter-03-synthetic-data","permalink":"/module-3-isaac-sim/chapter-03-synthetic-data","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 2: ROS 2 Integration and Robot Control","permalink":"/module-3-isaac-sim/chapter-02-ros2-integration"},"next":{"title":"Module 4: Vision-Language-Action (VLA) Models","permalink":"/module-4-vla/"}}');var a=t(4848),r=t(8453);const o={sidebar_position:3},s="Chapter 3: Synthetic Data Generation",d={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Why Synthetic Data?",id:"why-synthetic-data",level:2},{value:"Advantages",id:"advantages",level:3},{value:"Successful Applications",id:"successful-applications",level:3},{value:"Replicator Overview",id:"replicator-overview",level:2},{value:"Basic Data Generation Workflow",id:"basic-data-generation-workflow",level:2},{value:"Step 1: Create Scene",id:"step-1-create-scene",level:3},{value:"Step 2: Add Camera",id:"step-2-add-camera",level:3},{value:"Step 3: Define Randomization",id:"step-3-define-randomization",level:3},{value:"Step 4: Configure Output",id:"step-4-configure-output",level:3},{value:"Step 5: Run Generation",id:"step-5-run-generation",level:3},{value:"Domain Randomization Techniques",id:"domain-randomization-techniques",level:2},{value:"Object Randomization",id:"object-randomization",level:3},{value:"Lighting Randomization",id:"lighting-randomization",level:3},{value:"Background Randomization",id:"background-randomization",level:3},{value:"Camera Randomization",id:"camera-randomization",level:3},{value:"Complete Object Detection Dataset",id:"complete-object-detection-dataset",level:2},{value:"Pose Estimation Dataset",id:"pose-estimation-dataset",level:2},{value:"Training with Synthetic Data",id:"training-with-synthetic-data",level:2},{value:"Loading Data in PyTorch",id:"loading-data-in-pytorch",level:3},{value:"Mixing Synthetic and Real Data",id:"mixing-synthetic-and-real-data",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"1. Diversify Your Data",id:"1-diversify-your-data",level:3},{value:"2. Match Real-World Distribution",id:"2-match-real-world-distribution",level:3},{value:"3. Start Small, Scale Up",id:"3-start-small-scale-up",level:3},{value:"4. Version Your Datasets",id:"4-version-your-datasets",level:3},{value:"Summary",id:"summary",level:2},{value:"Further Reading",id:"further-reading",level:2}];function l(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"chapter-3-synthetic-data-generation",children:"Chapter 3: Synthetic Data Generation"})}),"\n",(0,a.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsx)(n.p,{children:"Training robust AI models for robotics requires massive amounts of labeled data. Collecting real-world data is expensive, time-consuming, and limited in diversity. Isaac Sim's synthetic data generation capabilities solve this problem by creating unlimited photorealistic training data with perfect ground truth labels."}),"\n",(0,a.jsx)(n.p,{children:"In this chapter, you'll learn how to use Isaac Sim's Replicator tool to generate synthetic datasets for object detection, segmentation, pose estimation, and more."}),"\n",(0,a.jsx)(n.h2,{id:"why-synthetic-data",children:"Why Synthetic Data?"}),"\n",(0,a.jsx)(n.h3,{id:"advantages",children:"Advantages"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Infinite Data"}),": Generate millions of training examples"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Perfect Labels"}),": Automatic ground truth for all tasks"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Domain Randomization"}),": Create robust models that generalize"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Rare Scenarios"}),": Test edge cases that rarely occur in reality"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Cost Effective"}),": No manual labeling required"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"successful-applications",children:"Successful Applications"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"NVIDIA DOPE"}),": 6D pose estimation trained entirely on synthetic data"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Isaac ROS Object Detection"}),": Retail object detection with 95%+ accuracy"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Warehouse Navigation"}),": Autonomous forklifts trained in simulation"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"replicator-overview",children:"Replicator Overview"}),"\n",(0,a.jsxs)(n.p,{children:["Isaac Sim's ",(0,a.jsx)(n.strong,{children:"Replicator"})," is a Python API for synthetic data generation. Key capabilities:"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Randomization"}),": Automatically vary objects, lighting, materials, camera angles"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Data Writers"}),": Export data in formats for PyTorch, TensorFlow, COCO, KITTI"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Annotations"}),": 2D/3D bounding boxes, segmentation, depth, normals"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Scalability"}),": Generate datasets in parallel"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"basic-data-generation-workflow",children:"Basic Data Generation Workflow"}),"\n",(0,a.jsx)(n.h3,{id:"step-1-create-scene",children:"Step 1: Create Scene"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import omni.replicator.core as rep\nfrom pxr import Gf\n\n# Create environment\nwith rep.new_layer():\n    # Ground plane\n    plane = rep.create.plane(\n        scale=10,\n        visible=True\n    )\n\n    # Create objects to detect\n    cube = rep.create.cube(\n        position=(0, 0, 1),\n        scale=0.5,\n        semantics=[("class", "cube")]\n    )\n\n    sphere = rep.create.sphere(\n        position=(1, 0, 0.5),\n        scale=0.3,\n        semantics=[("class", "sphere")]\n    )\n'})}),"\n",(0,a.jsx)(n.h3,{id:"step-2-add-camera",children:"Step 2: Add Camera"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# Create camera\ncamera = rep.create.camera(\n    position=(3, 3, 2),\n    look_at=(0, 0, 0.5)\n)\n\n# Attach render product\nrender_product = rep.create.render_product(camera, (1024, 1024))\n"})}),"\n",(0,a.jsx)(n.h3,{id:"step-3-define-randomization",children:"Step 3: Define Randomization"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'def randomize_scene():\n    # Randomize object positions\n    with cube:\n        rep.modify.pose(\n            position=rep.distribution.uniform((-2, -2, 0.5), (2, 2, 1.5)),\n            rotation=rep.distribution.uniform((0, 0, 0), (360, 360, 360))\n        )\n\n    with sphere:\n        rep.modify.pose(\n            position=rep.distribution.uniform((-2, -2, 0.5), (2, 2, 1.5))\n        )\n\n    # Randomize lighting\n    with rep.create.light():\n        rep.modify.attribute("intensity", rep.distribution.uniform(500, 3000))\n        rep.modify.attribute("color", rep.distribution.uniform((0.8, 0.8, 0.8), (1.0, 1.0, 1.0)))\n\n    return cube.node, sphere.node\n\n# Register randomizer\nrep.randomizer.register(randomize_scene)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"step-4-configure-output",children:"Step 4: Configure Output"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Basic writer (RGB + annotations)\nwriter = rep.WriterRegistry.get("BasicWriter")\n\nwriter.initialize(\n    output_dir="./synthetic_data",\n    rgb=True,\n    bounding_box_2d_tight=True,\n    semantic_segmentation=True,\n    instance_segmentation=True,\n    distance_to_camera=True\n)\n\nwriter.attach([render_product])\n'})}),"\n",(0,a.jsx)(n.h3,{id:"step-5-run-generation",children:"Step 5: Run Generation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# Generate 1000 frames\nwith rep.trigger.on_frame(num_frames=1000):\n    rep.randomizer.randomize_scene()\n\n# Execute (blocking)\nrep.orchestrator.run()\n"})}),"\n",(0,a.jsx)(n.h2,{id:"domain-randomization-techniques",children:"Domain Randomization Techniques"}),"\n",(0,a.jsx)(n.h3,{id:"object-randomization",children:"Object Randomization"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Randomize object appearance\ndef randomize_materials():\n    # Random colors\n    with cube:\n        rep.randomizer.color(\n            colors=rep.distribution.uniform((0, 0, 0), (1, 1, 1))\n        )\n\n    # Random textures\n    textures = [\n        "wood.png",\n        "metal.png",\n        "plastic.png",\n        "concrete.png"\n    ]\n\n    with sphere:\n        rep.randomizer.texture(\n            textures=rep.distribution.choice(textures)\n        )\n'})}),"\n",(0,a.jsx)(n.h3,{id:"lighting-randomization",children:"Lighting Randomization"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'def randomize_lighting():\n    # Random sun direction\n    sun = rep.create.light(\n        light_type="Distant",\n        intensity=rep.distribution.uniform(500, 2000),\n        rotation=rep.distribution.uniform((0, 0, 0), (90, 360, 0))\n    )\n\n    # Random point lights\n    num_lights = rep.distribution.choice([1, 2, 3])\n\n    for _ in range(num_lights):\n        rep.create.light(\n            light_type="Sphere",\n            position=rep.distribution.uniform((-5, -5, 2), (5, 5, 5)),\n            intensity=rep.distribution.uniform(1000, 5000),\n            color=rep.distribution.uniform((0.8, 0.8, 0.8), (1.0, 1.0, 1.0))\n        )\n'})}),"\n",(0,a.jsx)(n.h3,{id:"background-randomization",children:"Background Randomization"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Random background images\nbackgrounds = [\n    "indoor_1.hdr",\n    "outdoor_1.hdr",\n    "warehouse.hdr",\n    "office.hdr"\n]\n\ndef randomize_background():\n    bg = rep.distribution.choice(backgrounds)\n    rep.settings.set_render_settings(\n        dome_light_path=bg,\n        dome_light_intensity=rep.distribution.uniform(500, 1500)\n    )\n'})}),"\n",(0,a.jsx)(n.h3,{id:"camera-randomization",children:"Camera Randomization"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"def randomize_camera():\n    # Random camera position (spherical coordinates)\n    radius = rep.distribution.uniform(2, 5)\n    theta = rep.distribution.uniform(0, 360)\n    phi = rep.distribution.uniform(20, 80)\n\n    with camera:\n        rep.modify.pose(\n            position=(\n                radius * np.sin(np.radians(phi)) * np.cos(np.radians(theta)),\n                radius * np.sin(np.radians(phi)) * np.sin(np.radians(theta)),\n                radius * np.cos(np.radians(phi))\n            ),\n            look_at=(0, 0, 0.5)\n        )\n"})}),"\n",(0,a.jsx)(n.h2,{id:"complete-object-detection-dataset",children:"Complete Object Detection Dataset"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import omni.replicator.core as rep\nimport numpy as np\n\n# Scene setup\nwith rep.new_layer():\n    # Environment\n    ground = rep.create.plane(scale=20, visible=True)\n\n    # Objects to detect (multiple classes)\n    objects = []\n\n    # Class 1: Cubes\n    for i in range(5):\n        obj = rep.create.cube(\n            semantics=[("class", "box")],\n            scale=rep.distribution.uniform(0.3, 0.8)\n        )\n        objects.append(obj)\n\n    # Class 2: Cylinders\n    for i in range(5):\n        obj = rep.create.cylinder(\n            semantics=[("class", "bottle")],\n            scale=(0.1, 0.1, 0.3)\n        )\n        objects.append(obj)\n\n    # Class 3: Spheres\n    for i in range(5):\n        obj = rep.create.sphere(\n            semantics=[("class", "ball")],\n            scale=rep.distribution.uniform(0.2, 0.5)\n        )\n        objects.append(obj)\n\n    # Camera\n    camera = rep.create.camera(\n        position=(5, 5, 3),\n        look_at=(0, 0, 0)\n    )\n\n    render_product = rep.create.render_product(camera, (640, 480))\n\ndef randomize_scene():\n    # Randomize all object poses\n    for obj in objects:\n        with obj:\n            rep.modify.pose(\n                position=rep.distribution.uniform((-4, -4, 0.2), (4, 4, 2)),\n                rotation=rep.distribution.uniform((0, 0, 0), (360, 360, 360))\n            )\n\n            # Random colors\n            rep.randomizer.color(\n                colors=rep.distribution.uniform((0.1, 0.1, 0.1), (0.9, 0.9, 0.9))\n            )\n\n    # Randomize lighting\n    with rep.create.light(light_type="Dome"):\n        rep.modify.attribute("intensity", rep.distribution.uniform(800, 1500))\n\n    # Randomize camera\n    with camera:\n        rep.modify.pose(\n            position=rep.distribution.uniform((3, 3, 2), (7, 7, 5)),\n            look_at=(0, 0, 1)\n        )\n\n    return objects\n\nrep.randomizer.register(randomize_scene)\n\n# Configure writer for object detection\nwriter = rep.WriterRegistry.get("COCOWriter")  # COCO format\nwriter.initialize(\n    output_dir="./object_detection_dataset",\n    classes=["box", "bottle", "ball"]\n)\nwriter.attach([render_product])\n\n# Generate dataset\nwith rep.trigger.on_frame(num_frames=5000):\n    rep.randomizer.randomize_scene()\n\nrep.orchestrator.run()\n\nprint("Dataset generated at ./object_detection_dataset")\n'})}),"\n",(0,a.jsx)(n.h2,{id:"pose-estimation-dataset",children:"Pose Estimation Dataset"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# 6D pose estimation (position + orientation)\ndef generate_pose_dataset():\n    # Load target object (e.g., tool, product)\n    target_obj = rep.create.from_usd(\n        "path/to/object.usd",\n        semantics=[("class", "target_object")]\n    )\n\n    camera = rep.create.camera()\n    render_product = rep.create.render_product(camera, (1024, 1024))\n\n    def randomize():\n        with target_obj:\n            rep.modify.pose(\n                position=rep.distribution.uniform((-0.5, -0.5, 0.5), (0.5, 0.5, 1.5)),\n                rotation=rep.distribution.uniform((0, 0, 0), (360, 360, 360))\n            )\n\n        with camera:\n            rep.modify.pose(\n                position=rep.distribution.uniform((0.5, 0.5, 0.5), (2, 2, 2)),\n                look_at=target_obj\n            )\n\n    rep.randomizer.register(randomize)\n\n    # Writer with 3D bounding box\n    writer = rep.WriterRegistry.get("BasicWriter")\n    writer.initialize(\n        output_dir="./pose_dataset",\n        rgb=True,\n        bounding_box_3d=True,  # 6D pose\n        semantic_segmentation=True\n    )\n    writer.attach([render_product])\n\n    with rep.trigger.on_frame(num_frames=10000):\n        rep.randomizer.randomize()\n\n    rep.orchestrator.run()\n'})}),"\n",(0,a.jsx)(n.h2,{id:"training-with-synthetic-data",children:"Training with Synthetic Data"}),"\n",(0,a.jsx)(n.h3,{id:"loading-data-in-pytorch",children:"Loading Data in PyTorch"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import torch\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nimport json\n\nclass SyntheticDataset(Dataset):\n    def __init__(self, data_dir):\n        self.data_dir = data_dir\n        self.annotations = json.load(open(f\"{data_dir}/annotations.json\"))\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, idx):\n        ann = self.annotations[idx]\n\n        # Load image\n        img_path = f\"{self.data_dir}/{ann['image_path']}\"\n        image = Image.open(img_path).convert('RGB')\n\n        # Get bounding boxes\n        boxes = torch.tensor(ann['bounding_boxes'])\n        labels = torch.tensor(ann['labels'])\n\n        return image, boxes, labels\n\n# Usage\ndataset = SyntheticDataset(\"./object_detection_dataset\")\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n"})}),"\n",(0,a.jsx)(n.h3,{id:"mixing-synthetic-and-real-data",children:"Mixing Synthetic and Real Data"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Best practice: Start with 100% synthetic, gradually add real data\n\n# Stage 1: 100% synthetic (baseline)\nsynthetic_dataset = SyntheticDataset("./synthetic_data")\n\n# Stage 2: 90% synthetic, 10% real (fine-tuning)\nmixed_dataset = torch.utils.data.ConcatDataset([\n    synthetic_dataset,\n    real_dataset\n])\n\n# Stage 3: 50/50 or more real data (production model)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,a.jsx)(n.h3,{id:"1-diversify-your-data",children:"1. Diversify Your Data"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Multiple environments\nenvironments = ["warehouse", "office", "outdoor", "retail"]\n\nfor env in environments:\n    # Generate 1000 samples per environment\n    generate_dataset(env, num_samples=1000)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"2-match-real-world-distribution",children:"2. Match Real-World Distribution"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# If real data shows objects mostly upright, bias randomization:\ndef realistic_orientation():\n    # 80% upright, 20% arbitrary\n    if np.random.random() < 0.8:\n        rotation = rep.distribution.uniform((0, 0, 0), (15, 15, 360))\n    else:\n        rotation = rep.distribution.uniform((0, 0, 0), (360, 360, 360))\n\n    return rotation\n"})}),"\n",(0,a.jsx)(n.h3,{id:"3-start-small-scale-up",children:"3. Start Small, Scale Up"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# Generate small validation set first (100 samples)\n# Visually inspect quality\n# Then scale to full dataset (10k-1M samples)\n"})}),"\n",(0,a.jsx)(n.h3,{id:"4-version-your-datasets",children:"4. Version Your Datasets"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"output_dir = f\"./datasets/v1.0_{datetime.now().strftime('%Y%m%d')}\"\n"})}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(n.p,{children:"In this chapter, you learned:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Why synthetic data is essential for robot AI"}),"\n",(0,a.jsx)(n.li,{children:"Isaac Sim's Replicator tool for data generation"}),"\n",(0,a.jsx)(n.li,{children:"Domain randomization techniques (objects, lighting, camera, background)"}),"\n",(0,a.jsx)(n.li,{children:"Creating complete datasets for object detection and pose estimation"}),"\n",(0,a.jsx)(n.li,{children:"Training models with synthetic data in PyTorch"}),"\n",(0,a.jsx)(n.li,{children:"Best practices for high-quality synthetic datasets"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Synthetic data generation is a game-changer for robotics AI. With Isaac Sim, you can create unlimited training data that would be impossible or prohibitively expensive to collect in the real world."}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Module 3 Complete!"})," You now understand Isaac Sim's powerful capabilities for simulation, ROS 2 integration, and synthetic data generation."]}),"\n",(0,a.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Replicator Documentation: ",(0,a.jsx)(n.a,{href:"https://docs.omniverse.nvidia.com/py/replicator/",children:"https://docs.omniverse.nvidia.com/py/replicator/"})]}),"\n",(0,a.jsx)(n.li,{children:'Domain Randomization Paper: Tobin et al. (2017). "Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World."'}),"\n",(0,a.jsxs)(n.li,{children:["NVIDIA DOPE: ",(0,a.jsx)(n.a,{href:"https://github.com/NVlabs/Deep_Object_Pose",children:"https://github.com/NVlabs/Deep_Object_Pose"})]}),"\n",(0,a.jsxs)(n.li,{children:["Isaac ROS: ",(0,a.jsx)(n.a,{href:"https://github.com/NVIDIA-ISAAC-ROS",children:"https://github.com/NVIDIA-ISAAC-ROS"})]}),"\n"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Learning Check"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"\u2713 Use Replicator for synthetic data generation"}),"\n",(0,a.jsx)(n.li,{children:"\u2713 Apply domain randomization techniques"}),"\n",(0,a.jsx)(n.li,{children:"\u2713 Generate object detection datasets"}),"\n",(0,a.jsx)(n.li,{children:"\u2713 Create pose estimation training data"}),"\n",(0,a.jsx)(n.li,{children:"\u2713 Train models with synthetic data"}),"\n",(0,a.jsx)(n.li,{children:"\u2713 Follow best practices for data quality"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(l,{...e})}):l(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>s});var i=t(6540);const a={},r=i.createContext(a);function o(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);